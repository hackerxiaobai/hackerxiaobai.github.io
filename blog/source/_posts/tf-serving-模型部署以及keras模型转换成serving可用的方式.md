---
title: tf-serving æ¨¡å‹éƒ¨ç½²ä»¥åŠkerasæ¨¡å‹è½¬æ¢æˆservingå¯ç”¨çš„æ–¹å¼
date: 2020-07-31 20:11:12
tags:
	- tf-serving
	- keras 
	- model è½¬æˆ serving
---

### tf-serving æ¨¡å‹éƒ¨ç½²

+ ç”±äºä¹‹å‰ä¸€ç›´éƒ½æ˜¯ä½¿ç”¨pytorch,éƒ¨ç½²å°±ä¸€ç›´ä¹Ÿæ²¡å»å…³å¿ƒtf-servingé‚£ä¸€å¥—é¬¼ä¸œè¥¿,è¿‘æœŸä½¿ç”¨äº†ä¸€æ®µæ—¶é—´tf,keras,æ¶‰åŠåˆ°éƒ¨ç½²é—®é¢˜,å½“ç„¶è¿™å°±ç¦»ä¸å¼€servingäº†,æ‰€ä»¥å°±åšä¸€ä¸ªä½¿ç”¨æ€»ç»“,ä¾›è‡ªå·±åç»­å‚è€ƒå§,åˆ«åˆå¿˜è®°äº†
+ ç½‘ä¸Šèµ„æ–™ä¸€å¤§å †,å¦‚æœä½ æ˜¯ç†Ÿæ‚‰è¿™äº›ä¸œè¥¿çš„,æ²¡æœ‰å¿…è¦å¾€ä¸‹çœ‹äº†
+ é‚£å°±å…ˆä»ä¸€èˆ¬çš„ tfæ¨¡å‹ä»£ç è¯´èµ·å§

<!-- more -->

> ç®€å•å®šä¹‰ä¸€ä¸ªtfæ¨¡å‹

```python
import os
import shutil
 
import numpy as np
import tensorflow as tf
 
 
def add_layer(inputs, input_size, output_size, activation_function=None):
    weights = tf.Variable(tf.random_normal([input_size, output_size]))
    biases = tf.Variable(tf.zeros([1, output_size]) + 0.1)
    wx_plus_b = tf.matmul(inputs, weights) + biases  # WX + b
    if activation_function is None:
        outputs = wx_plus_b
    else:
        outputs = activation_function(wx_plus_b)
    return outputs
 
 
# é€ ä¸€äº›éšæœºè¾“å…¥æ•°æ®
num_points = 30000  # æ€»æ•°æ®æ¡æ•°
feature_number = 100  # æ¯æ¡è¾“å…¥æ•°æ®æœ‰100ä¸ªfeature
# num_pointsä¸ªè¾“å…¥æ•°æ®,æ¯ä¸ªæœ‰feature_numberä¸ªfeature,å³è¾“å…¥æ•°æ®çš„ç»´åº¦æ˜¯(num_points,feature_number)
x_data = np.random.rand(num_points, feature_number)
y_data = np.random.randint(0, 2, (num_points, 1))  # nx1çš„æ•°ç»„, æ¯ä¸€è¡Œä¸º1ä¸ªæ•°(0æˆ–1)
 
# ç”¨äºæ¥æ”¶è¾“å…¥çš„Tensor
x_actual = tf.placeholder(tf.float32, [None, feature_number], name="myInput")
y_actual = tf.placeholder(tf.float32, [None, 1], name="myOutput")
 
# éšå±‚1
l1 = add_layer(x_actual, feature_number, 32, activation_function=tf.nn.relu)
# éšå±‚2
l2 = add_layer(l1, 32, 64, activation_function=tf.nn.tanh)
# éšå±‚3
l3 = add_layer(l2, 64, 32, activation_function=tf.nn.relu)

# è¾“å‡ºå±‚
y_predict = add_layer(l3, 32, 1, activation_function=tf.nn.sigmoid)
# æŸå¤±å‡½æ•°
loss = -tf.reduce_mean(y_actual * tf.log(tf.clip_by_value(y_predict, 1e-10, 1.0)))
# ä¼˜åŒ–å™¨
train_step = tf.train.AdamOptimizer(0.001).minimize(loss)
 
init = tf.global_variables_initializer()
# è¿­ä»£æ¬¡æ•°
num_iterations = 10000
with tf.Session() as sess:
    sess.run(init)
    for i in range(num_iterations):
        # è®­ç»ƒæ¨¡å‹
        sess.run(train_step, feed_dict={x_actual: x_data, y_actual: y_data})
        if i % 500 == 0:
            prediction_value = sess.run(y_predict, feed_dict={x_actual: x_data})
            print(sess.run(loss, feed_dict={x_actual: x_data, y_actual: y_data}))
    # è®­ç»ƒå®Œæˆå,ä»¥SavedModelæ ¼å¼ä¿å­˜æ¨¡å‹æ–‡ä»¶
    model_output_dir = "./model/201908070001"
    if os.path.exists(model_output_dir):  # ç›®å½•å­˜åœ¨
        shutil.rmtree(model_output_dir)  # åˆ é™¤åŸç›®å½•
    tf.saved_model.simple_save(
        sess, model_output_dir, inputs={"myInput": x_actual}, outputs={"myOutput": y_predict})
```

+ ä¿å­˜æ¨¡å‹çš„ç›®å½•å¤§æ¦‚æ˜¯æœ‰è¿™å‡ ä¸ªæ–‡ä»¶

{% asset_img 1.jpg è¯´æ˜ %}

+ æ¥ä¸‹æ¥æ‹‰ä¸€ä¸ªTensorFlow-Servingçš„Dockeré•œåƒ
  + **docker pull tensorflow/serving**

+ ä¸‹è½½å®Œåï¼Œå† cd åˆ°ä¸Šé¢è¾“å‡ºçš„â€œmodelâ€ç›®å½•çš„ä¸Šä¸€çº§ç›®å½•,è¿è¡Œä¸‹é¢çš„å‘½ä»¤

  + TESTDATA="$(pwd)/model"

    docker run -t --rm -p 8501:8501 \

      -v "$TESTDATA:/models/simple_fc_nn" \

      -e MODEL_NAME=simple_fc_nn \

      tensorflow/serving 

+ è¿™æ ·å°±å¯ä»¥æŠŠæ¨¡å‹serveèµ·æ¥äº†ã€‚å…¶ä¸­ï¼Œç«¯å£å·å¯ä»¥è‡ªå·±æ”¹ï¼Œsimple_fc_nnæ˜¯æˆ‘è‡ªå·±èµ·çš„æ¨¡å‹åç§°ï¼Œåœ¨åé¢ä½¿ç”¨REST APIæ¥è®¿é—®TF-ServingæœåŠ¡çš„æ—¶å€™ï¼Œä¼šç”¨åˆ°è¿™ä¸ªåç§°ã€‚
+ é€šè¿‡REST APIæŸ¥çœ‹æœåŠ¡çŠ¶æ€
  + **curl http://localhost:8501/v1/models/simple_fc_nn**
+ è¿™é‡Œå¯ä»¥çœ‹åˆ°ï¼ŒURLé‡Œçš„simple_fc_nnå°±æ˜¯åœ¨å¯åŠ¨TF-ServingæœåŠ¡çš„æ—¶å€™æŒ‡å®šçš„é‚£ä¸ªåå­—ã€‚
  å¦å¤–ï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯localhostï¼Œæ‰€ä»¥å¿…é¡»åœ¨TF-Servingè¿è¡Œçš„åŒä¸€å°æœºå™¨ä¸Šæ‰§è¡Œè¯¥å‘½ä»¤ã€‚
  è¿”å›ï¼š
  + {% asset_img 2.jpg è¯´æ˜ %}
+ é€šè¿‡REST APIæŸ¥çœ‹æ¨¡å‹çš„å…ƒæ•°æ®
  + **curl http://localhost:8501/v1/models/simple_fc_nn/metadata**
+ é€šè¿‡Apache abå¯¹TF-Servingè¿›è¡Œæ€§èƒ½æµ‹è¯• post.txt é‡Œæ”¾çš„æ˜¯è¾“å…¥æ•°æ®,(è½¬æˆjsonçš„æ ¼å¼)
  + **ab -n 100000 -c 50 -T 'Content-Type:application/json' -p ./post.txt http://localhost:8501/v1/models/simple_fc_nn:predict**



### keras æ¨¡å‹ è½¬åˆ° tf-serving å¯ç”¨çš„pbæ ¼å¼

+ ç›´æ¥çœ‹ä»£ç å§

```python
def keras_model_2_tf_serving():
    '''
	    keras modelä¼šè‡ªåŠ¨ä¿å­˜ä¸ºpbæ ¼å¼
	'''
    def export_model(model,export_model_dir,model_version):
        """
        :param export_model_dir: type string, save dir for exported model    url
        :param model_version: type int best
        :return:no return
        """
        with tf.get_default_graph().as_default():
            # prediction_signature
            tensor_info_input_0 = tf.saved_model.utils.build_tensor_info(model.input[0])
            tensor_info_input_1 = tf.saved_model.utils.build_tensor_info(model.input[1])
            tensor_info_output = tf.saved_model.utils.build_tensor_info(model.output)
            prediction_signature = (
                tf.saved_model.signature_def_utils.build_signature_def(
                    inputs ={'input_0': tensor_info_input_0,'input_1': tensor_info_input_1}, # Tensorflow.TensorInfo
                    outputs={'result': tensor_info_output},
                    #method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)
                    method_name= "tensorflow/serving/predict")
                
            )
            print('step1 => prediction_signature created successfully')
            # set-up a builder
            os.mkdir(export_model_dir)
            export_path_base = export_model_dir
            export_path = os.path.join(
                tf.compat.as_bytes(export_path_base),
                tf.compat.as_bytes(str(model_version)))
            builder = tf.saved_model.builder.SavedModelBuilder(export_path)
            builder.add_meta_graph_and_variables(
                # tags:SERVING,TRAINING,EVAL,GPU,TPU
                sess=K.get_session(),
                tags=[tf.saved_model.tag_constants.SERVING],
                clear_devices=True,
                signature_def_map={
                    'predict':prediction_signature,
                    tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: prediction_signature,

                },
                )
            print('step2 => Export path(%s) ready to export trained model' % export_path, '\n starting to export model...')
            #builder.save(as_text=True)
            builder.save()
            print('Done exporting!')


    with open('model_config_classification.json') as json_file:
        json_config = json_file.read()
    model = keras.models.model_from_json(json_config)
    model.load_weights('model_weights_classification.h5')

    # å¯¼å‡ºtf-servingçš„æ–¹å¼
    export_model(model,"tf-serving-model",1)

# é€šè¿‡ä¸Šé¢çš„è½¬æ¢,å¯ä»¥ç”¨tf-servingå¯èµ·æ¥
'''
TESTDATA="$(pwd)/tf-serving-model"
docker run -t --rm -p 8503:8501 \
    -v "$TESTDATA:/models/bert" \
    -e MODEL_NAME=bert \
    tensorflow/serving
'''
    
def predict_tf_serving():
    sent = "æ¥ç©è‹±é›„è”ç›Ÿ"
    tokenid_train =  tokenizer.encode(sent,maxlen=200)[0] 
    sen_id_train =   tokenizer.encode(sent,maxlen=200)[1]
    SERVER_URL = "http://localhost:8503/v1/models/bert:predict"
    predict_request='{"signature_name": "predict", "instances":[{"input_0":%s,"input_1":%s}] }' %(tokenid_train,sen_id_train)
    response = requests.post(SERVER_URL, data=predict_request)
    res = json.loads(response.content)
    print(res)
```

+ å¤§æ¦‚å°±æ˜¯è¿™ä¹ˆä¸€ä¸ªæ ·å­å§,çœ‹æ‡‚å°±ğŸ‘Œ

