<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/NLP.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/NLP.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Pointer-Network,Summarization,seq2seq,copy,">










<meta name="description" content="之前或多或少都有去关注以及该方面的paper阅读，但是并没有去好好的整理该技术的整体发展，今天闲来无事，想从代码以及paper的核心思想梳理一遍。供自己后续方便查看吧。  Pointer Networks">
<meta name="keywords" content="Pointer-Network,Summarization,seq2seq,copy">
<meta property="og:type" content="article">
<meta property="og:title" content="seq2seq Pointer-Network Copy 等技术梳理">
<meta property="og:url" content="http://yoursite.com/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/index.html">
<meta property="og:site_name" content="王磊的博客">
<meta property="og:description" content="之前或多或少都有去关注以及该方面的paper阅读，但是并没有去好好的整理该技术的整体发展，今天闲来无事，想从代码以及paper的核心思想梳理一遍。供自己后续方便查看吧。  Pointer Networks">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/1.png">
<meta property="og:image" content="http://yoursite.com/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/2.png">
<meta property="og:image" content="http://yoursite.com/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/3.png">
<meta property="og:updated_time" content="2020-06-14T10:22:15.252Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="seq2seq Pointer-Network Copy 等技术梳理">
<meta name="twitter:description" content="之前或多或少都有去关注以及该方面的paper阅读，但是并没有去好好的整理该技术的整体发展，今天闲来无事，想从代码以及paper的核心思想梳理一遍。供自己后续方便查看吧。  Pointer Networks">
<meta name="twitter:image" content="http://yoursite.com/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/">





  <title>seq2seq Pointer-Network Copy 等技术梳理 | 王磊的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">王磊的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">凡所有相，皆是虚妄</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-github">
          <a href="https://github.com/hackerxiaobai" rel="section">
            
            github
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="王磊">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="王磊的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">seq2seq Pointer-Network Copy 等技术梳理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-03T15:17:07+08:00">
                2020-06-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>之前或多或少都有去关注以及该方面的paper阅读，但是并没有去好好的整理该技术的整体发展，今天闲来无事，想从代码以及paper的核心思想梳理一遍。供自己后续方便查看吧。</p>
</blockquote>
<h2 id="Pointer-Networks"><a href="#Pointer-Networks" class="headerlink" title="Pointer Networks"></a>Pointer Networks</h2><a id="more"></a>

<p><strong><a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">paper</a></strong></p>
<p><strong><a href="https://github.com/shirgur/PointerNet" target="_blank" rel="noopener">pytorch code</a></strong></p>
<p>传统的seq2seq模型是无法解决输出序列的词汇表会随着输入序列长度的改变而改变的问题，所以一个很简单的想法就是为什么输出不能直接从输入端直接拿过来呢？那直接从输入端拿过来，具体要怎么操作呢？这就是该篇文章给出的一个思路，该篇文章给出的形象解释是凸包问题，如图所示</p>
<img src="/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/1.png" title="1.png">

<p>对上图的一个简单解释：给定p1到p4四个二维坐标，找到一个凸包。答案是p1-&gt;p4-&gt;p2-&gt;p1，图a就是传统的seq2seq做法，就是把四个点的坐标作为输入序列输入进去，然后提供一个词汇表：[start, 1, 2, 3, 4, end]，最后依据词汇表预测出序列[start, 1, 4, 2, 1, end]，缺点作者也提到过了，对于图a的传统seq2seq模型来说，它的输出词汇表已经限定，当输入序列的长度变化的时候（如变为10个点）它根本无法预测大于4的数字。因为你的词汇表限定了最大就是4。图b是作者提出的Pointer Networks，它预测的时候每一步都找当前输入序列中权重最大的那个元素，而由于输出序列完全来自输入序列，它可以适应输入序列的长度变化。那具体的是怎么处理的呢？下面就直接从代码实现层面来简单说一下。</p>
<p><strong>还是以解凸包问题说起</strong></p>
<p>每一个batch5个坐标点，那最开始的输入就是：(假设batch 256)</p>
<p>inputs.shape (256，5，2)</p>
<p>假设embedding是128，那inputs 经过embedding后的shape就是：</p>
<p>embedded_inputs（256，5，128）</p>
<p>然后进行encode，假设用了LSTM，（uints假设为512）那它会输出 encoder_outputs 和 encoder_hidden，shape分别是：</p>
<p>encoder_outputs（256, 5, 512） </p>
<p>encoder_hidden（256, 512）</p>
<p>接下来我们就要开始decode了，重点就是decode端去实现如何直接拿输入的信息了，其实对于这种seq2seq现在都会做一个attention的操作，那该paper其实就是在attention上做了简化，通过attention的操作得到一个alpha，通过alpha间接去拿输入端embedded_inputs 的具体某一个坐标的embedding。下面看一下decode端的一个操作吧：decode的输入主要是这四个值 </p>
<p>embedded_inputs   （256, 5, 128）就是encode端的embedding</p>
<p>decoder_input0  （256, 128）因为是t0时刻，所以这个值最开始是随机初始化的</p>
<p>decoder_hidden0 （256, 512）就是拿了encode端最后一个时刻的隐状态作为decode端的开始状态</p>
<p>encoder_outputs （256, 5, 512） </p>
<p>将 decoder_input0 和 decoder_hidden0 经过一个时刻的LSTM操作得到t1 的 h_t, 然后将 encoder_outputs 和 t_1 时刻的h_t 输入到attention，此时attention操作就是计算出一个alpha。具体如何计算呢，继续往下看：</p>
<p>我们知道上面操作得到的 h_t维度是（256，512）， encoder_outputs 维度是（256, 5, 512） ，我们将h_t 进行repeat操作，维度变成 （256, 5, 512），h_t 和 encoder_outputs做一下维度变换，变成（256，512，5），然后这个encoder_outputs进行一次Conv1d操作，其实做不做这个操作我觉得影响也不是很大，该操作是不改变维度的，所以经过Conv1d操作后维度还是（256, 5, 512），在attention这里呢我们会一开始初始化一个变量，假设是V吧，，他的维度呢就是（256，512）的矩阵。（该矩阵呢其实是为了后面计算得到alpha的一个中间变量吧）为了矩阵操作方便，我们会将V进行维度扩展，变成（256，1，512），然后做一个这样的操作 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">att = torch.bmm(V, self.tanh(h_t+ encoder_outputs)).squeeze(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>所以此时得到的 att 的维度就是 （256，5），此时呢，直接对这个att 进行 softmax操作，得到alpha，然后将 alpha 和 encoder_outputs做一个计算，得到一个 hidden_state </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hidden_state = torch.bmm(encoder_outputs, alpha.unsqueeze(<span class="number">2</span>)).squeeze(<span class="number">2</span>)  </span><br><span class="line"><span class="comment"># hidden_state shape is （256，512）</span></span><br></pre></td></tr></table></figure>

<p>至此，attention操作就结束了，最后返回的就是 alpha 和 hidden_state</p>
<p>那到此呢，我们只是拿到了alpha而已，那怎么通过这个alpha直接到embedded_inputs去拿对应索引的embeding呢？接着往下看：</p>
<p>拿到的alpha会做一个max操作，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">max_probs, indices = alpha.max(<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 因为是输出随着输入变化而变化，所以一开始呢，我们会初始化一个runner，</span></span><br><span class="line"><span class="comment"># 比如我们这里坐标的个数采用了5，所以runner呢就会初始化成 [0,1,2,3,4] 的列表，</span></span><br><span class="line"><span class="comment"># 然后将它repeat到batch_size 大小，所以 runner的shape就是（256，5），然后呢，做一个这样操作：</span></span><br><span class="line"></span><br><span class="line">one_hot_pointers = (runner == indices.unsqueeze(<span class="number">1</span>).expand(<span class="number">-1</span>, alpha.size()[<span class="number">1</span>])).float()</span><br><span class="line"><span class="comment"># 所以此时呢 one_hot_pointers 就是一个 0/1 矩阵，其实就是 5个坐标 对应取哪一个坐标的索引嘛</span></span><br><span class="line"><span class="comment"># 然后通过这个矩阵到 embedded_inputs去挑一些embedding来做decode端 t1 时刻的 输入啦，具体操作如下：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get embedded inputs by max indices             </span></span><br><span class="line">embedding_mask = one_hot_pointers.unsqueeze(<span class="number">2</span>).expand(<span class="number">-1</span>, <span class="number">-1</span>, self.embedding_dim).byte()             decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)</span><br><span class="line"><span class="comment"># 从这里我们可以看大下一个时刻的 decoder_input 其实是直接通过alpha取max后对应的索引到 </span></span><br><span class="line"><span class="comment"># embedded_inputs直接拿到的。t2、t3...时刻以此类推，都是这样的操作</span></span><br></pre></td></tr></table></figure>

<p>到现在为止，已经解释了如何直接从输入端来取embedding来做为decode端的输入了，但是最终我们要拿到这个凸包的输出还没有解释，下面就简单来看一下吧，其实很简单了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">outputs.append(alpha.unsqueeze(<span class="number">0</span>))             </span><br><span class="line">pointers.append(indices.unsqueeze(<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 我们的坐标个数取的是5，所以其实是会循环5次上面这个操作，每一步拿到一个，最后cat起来</span></span><br><span class="line"></span><br><span class="line">outputs = torch.cat(outputs).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)    <span class="comment"># （256，5，5）     </span></span><br><span class="line">pointers = torch.cat(pointers, <span class="number">1</span>)  <span class="comment"># （256，5）</span></span><br></pre></td></tr></table></figure>

<p>最终，返回的outputs 会参与模型计算loss，至此，整个 Pointer Network 的代码实现就解释完了。或许有点懵。直接阅读代码吧，配合这个解释会特别清晰<br><strong>阅读code</strong></p>
<p><strong>阅读code</strong></p>
<p><strong>阅读code</strong></p>
<p>重要的事情说三遍，Down</p>
<h2 id="Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"><a href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks" class="headerlink" title="Get To The Point: Summarization with Pointer-Generator Networks"></a>Get To The Point: Summarization with Pointer-Generator Networks</h2><p><a href="https://arxiv.org/pdf/1704.04368.pdf" target="_blank" rel="noopener">paper</a></p>
<p><a href="https://github.com/atulkum/pointer_summarizer" target="_blank" rel="noopener">code</a></p>
<p>在这篇论文中，作者认为，用于文本摘要的seq2seq模型往往存在两大缺陷：</p>
<ul>
<li>模型容易不准确地再现事实细节，也就是说模型生成的摘要不准确；</li>
<li>往往会重复，也就是会重复生成一些词或者句子。而针对这两种缺陷，作者分别使用Pointer Networks和Coverage技术来解决</li>
<li>作者给了一张效果图如下：</li>
<li><img src="/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/2.png" title="2.png">



</li>
</ul>
<p>在这张图中，基础的seq2seq模型的预测结果存在许多谬误的句子，同时如nigeria这样的单词反复出现（红色部分）。这也就印证了作者提出的基础seq2seq在文本摘要时存在的问题；Pointer-Generator模型，也就是在seq2seq基础上加上Pointer Networks的模型基本可以做到不出现事实性的错误，但是重复预测句子的问题仍然存在（绿色部分）；最后，在Pointer-Generator模型上增加Coverage机制，可以看出，这次模型预测出的摘要不仅做到了事实正确，同时避免了重复某些句子的问题（摘要结果来自原文中的蓝色部分）</p>
<blockquote>
<p>那么，Pointer-Generator模型以及变体Pointer-Generator+Coverage模型是怎么做的呢，我们具体从代码层面来分析一下</p>
</blockquote>
<p>既然是Pointer Network的进一步改进，那首先想到的就是，如何像Pointer Network那样输出能跟着输入的改变而改变吧？为什么要有这样的操作，其实就是为了解决OOV的问题嘛，假设词表是10000，当你输入的某一个词不在词表中的时候，是不是就要用UNK来代替了，而且这个词也出现在decode端，那么decode端也是UNK了。所以为了解决这个问题，就有了，词表随着输入的扩大而扩大，具体代码体现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">article2ids</span><span class="params">(article_words, vocab)</span>:</span></span><br><span class="line">  ids = []</span><br><span class="line">  oovs = []</span><br><span class="line">  unk_id = vocab.word2id(UNKNOWN_TOKEN)</span><br><span class="line">  <span class="keyword">for</span> w <span class="keyword">in</span> article_words:</span><br><span class="line">    i = vocab.word2id(w)</span><br><span class="line">    <span class="keyword">if</span> i == unk_id: <span class="comment"># If w is OOV</span></span><br><span class="line">      <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> oovs: <span class="comment"># Add to list of OOVs</span></span><br><span class="line">        oovs.append(w)</span><br><span class="line">      oov_num = oovs.index(w) <span class="comment"># This is 0 for the first article OOV, 1 for the second article OOV...</span></span><br><span class="line">      ids.append(vocab.size() + oov_num) <span class="comment"># This is e.g. 50000 for the first article OOV, 50001 for the second...</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      ids.append(i)</span><br><span class="line">  <span class="keyword">return</span> ids, oovs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abstract2ids</span><span class="params">(abstract_words, vocab, article_oovs)</span>:</span></span><br><span class="line">  ids = []</span><br><span class="line">  unk_id = vocab.word2id(UNKNOWN_TOKEN)</span><br><span class="line">  <span class="keyword">for</span> w <span class="keyword">in</span> abstract_words:</span><br><span class="line">    i = vocab.word2id(w)</span><br><span class="line">    <span class="keyword">if</span> i == unk_id: <span class="comment"># If w is an OOV word</span></span><br><span class="line">      <span class="keyword">if</span> w <span class="keyword">in</span> article_oovs: <span class="comment"># If w is an in-article OOV</span></span><br><span class="line">        vocab_idx = vocab.size() + article_oovs.index(w) <span class="comment"># Map to its temporary article OOV number</span></span><br><span class="line">        ids.append(vocab_idx)</span><br><span class="line">      <span class="keyword">else</span>: <span class="comment"># If w is an out-of-article OOV</span></span><br><span class="line">        ids.append(unk_id) <span class="comment"># Map to the UNK token id</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      ids.append(i)</span><br><span class="line">  <span class="keyword">return</span> ids</span><br></pre></td></tr></table></figure>

<p><strong>从上面代码可以很清晰的看到，但你输入的词超过词表时，问题也不大，词表跟着扩大就行了。</strong></p>
<p>接着，现在是可以做到词表跟着输入的变化而变化了，但是接下来要怎么做呢？我们知道正常的seq2seq，encode端将词变成索引只要做这样一个操作就是了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.enc_input = [vocab.word2id(w) <span class="keyword">for</span> w <span class="keyword">in</span> article_words]</span><br></pre></td></tr></table></figure>

<p>这种操作当出现一个词不在词表中时，就会出现UNK对应的索引了。如果是这样来实现的话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> config.pointer_gen:</span><br><span class="line">      <span class="comment"># Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id; also store the in-article OOVs words themselves</span></span><br><span class="line">      self.enc_input_extend_vocab, self.article_oovs = data.article2ids(article_words, vocab)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id</span></span><br><span class="line">      abs_ids_extend_vocab = data.abstract2ids(abstract_words, vocab, self.article_oovs)</span><br></pre></td></tr></table></figure>

<p>因为词表跟着输入的扩充变化而变化，所以可以知道 <strong>self.enc_input_extend_vocab</strong> 列表里是不会出现UNK对应的索引的，至此输入端的情况就应该很清楚了。接着就是<strong>encode</strong>  <strong>decode</strong> 的一些操作了</p>
<p>encode端其实是很简单的一些操作，核心代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, seq_lens)</span>:</span></span><br><span class="line">    <span class="comment"># input.shape (batch_size, seq_length)</span></span><br><span class="line">    embedded = self.embedding(input)</span><br><span class="line">    packed = pack_padded_sequence(embedded, seq_lens, batch_first=<span class="literal">True</span>)</span><br><span class="line">    output, hidden = self.lstm(packed)</span><br><span class="line">    encoder_outputs, _ = pad_packed_sequence(output, batch_first=<span class="literal">True</span>)  <span class="comment"># h dim = B x t_k x n</span></span><br><span class="line">    encoder_outputs = encoder_outputs.contiguous()</span><br><span class="line">    encoder_feature = encoder_outputs.view(<span class="number">-1</span>, <span class="number">2</span>*config.hidden_dim)  <span class="comment"># B * t_k x 2*hidden_dim</span></span><br><span class="line">    <span class="comment"># [8, 400, 512] [3200, 512] [[2, 8, 256],[2, 8, 256]]</span></span><br><span class="line">    <span class="keyword">return</span> encoder_outputs, encoder_feature, hidden</span><br></pre></td></tr></table></figure>

<p>decode端稍微复杂一些，但是其实和Pointer Network 没什么的大的区别，也是在Attention操作的时候，直接拿到一个 （batch_size，seq_length)的概率分布矩阵，直接softmax操作，作为概率返回，看一下attention的核心代码吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, s_t_hat, encoder_outputs, encoder_feature, enc_padding_mask, coverage)</span>:</span></span><br><span class="line">        b, t_k, n = list(encoder_outputs.size())</span><br><span class="line">        dec_fea = self.decode_proj(s_t_hat) <span class="comment"># B x 2*hidden_dim</span></span><br><span class="line">        dec_fea_expanded = dec_fea.unsqueeze(<span class="number">1</span>).expand(b, t_k, n).contiguous() <span class="comment"># B x t_k x 2*hidden_dim</span></span><br><span class="line">        dec_fea_expanded = dec_fea_expanded.view(<span class="number">-1</span>, n)  <span class="comment"># B * t_k x 2*hidden_dim</span></span><br><span class="line"></span><br><span class="line">        att_features = encoder_feature + dec_fea_expanded <span class="comment"># B * t_k x 2*hidden_dim</span></span><br><span class="line">        <span class="keyword">if</span> config.is_coverage:</span><br><span class="line">            coverage_input = coverage.view(<span class="number">-1</span>, <span class="number">1</span>)  <span class="comment"># B * t_k x 1</span></span><br><span class="line">            coverage_feature = self.W_c(coverage_input)  <span class="comment"># B * t_k x 2*hidden_dim</span></span><br><span class="line">            att_features = att_features + coverage_feature</span><br><span class="line"></span><br><span class="line">        e = F.tanh(att_features) <span class="comment"># B * t_k x 2*hidden_dim</span></span><br><span class="line">        scores = self.v(e)  <span class="comment"># B * t_k x 1</span></span><br><span class="line">        scores = scores.view(<span class="number">-1</span>, t_k)  <span class="comment"># B x t_k</span></span><br><span class="line"></span><br><span class="line">        attn_dist_ = F.softmax(scores, dim=<span class="number">1</span>)*enc_padding_mask <span class="comment"># B x t_k</span></span><br><span class="line">        normalization_factor = attn_dist_.sum(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        attn_dist = attn_dist_ / normalization_factor</span><br><span class="line"></span><br><span class="line">        attn_dist = attn_dist.unsqueeze(<span class="number">1</span>)  <span class="comment"># B x 1 x t_k</span></span><br><span class="line">        c_t = torch.bmm(attn_dist, encoder_outputs)  <span class="comment"># B x 1 x n</span></span><br><span class="line">        c_t = c_t.view(<span class="number">-1</span>, config.hidden_dim * <span class="number">2</span>)  <span class="comment"># B x 2*hidden_dim</span></span><br><span class="line"></span><br><span class="line">        attn_dist = attn_dist.view(<span class="number">-1</span>, t_k)  <span class="comment"># B x t_k</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> config.is_coverage:</span><br><span class="line">            coverage = coverage.view(<span class="number">-1</span>, t_k)</span><br><span class="line">            coverage = coverage + attn_dist</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> c_t, attn_dist, coverage</span><br></pre></td></tr></table></figure>

<p>可以看到<strong>attn_dist</strong>其实和Pointer Network的实现或者说操作是一样的，没有什么大的区别，coverage是为了惩罚重复出现问题而计算的一个矩阵</p>
<p>在attention计算完毕，decode端是如何来计算整体的概率分布呢？还是直接看代码吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">p_gen = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> config.pointer_gen:</span><br><span class="line">  p_gen_input = torch.cat((c_t, s_t_hat, x), <span class="number">1</span>)  <span class="comment"># B x (2*2*hidden_dim + emb_dim)</span></span><br><span class="line">  p_gen = self.p_gen_linear(p_gen_input)</span><br><span class="line">  p_gen = F.sigmoid(p_gen)</span><br><span class="line"></span><br><span class="line">output = torch.cat((lstm_out.view(<span class="number">-1</span>, config.hidden_dim), c_t), <span class="number">1</span>) <span class="comment"># B x hidden_dim * 3</span></span><br><span class="line">output = self.out1(output) <span class="comment"># B x hidden_dim</span></span><br><span class="line">output = self.out2(output) <span class="comment"># B x vocab_size</span></span><br><span class="line">vocab_dist = F.softmax(output, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> config.pointer_gen:</span><br><span class="line">  vocab_dist_ = p_gen * vocab_dist</span><br><span class="line">  attn_dist_ = (<span class="number">1</span> - p_gen) * attn_dist</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> extra_zeros <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    vocab_dist_ = torch.cat([vocab_dist_, extra_zeros], <span class="number">1</span>)</span><br><span class="line">    final_dist = vocab_dist_.scatter_add(<span class="number">1</span>, enc_batch_extend_vocab, attn_dist_)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  final_dist = vocab_dist</span><br></pre></td></tr></table></figure>

<p>至此，我觉得我自己应该大差不差的可以搞清楚了。如果你没有搞明白，还是那句话，代码面前没有秘密。看代码去吧（插一句，这份代码中有好几个地方我觉得是有待考虑的，在实现上，但是主体还是OK的）</p>
<h2 id="CopyNet"><a href="#CopyNet" class="headerlink" title="CopyNet"></a>CopyNet</h2><p><a href="https://arxiv.org/pdf/1603.06393.pdf" target="_blank" rel="noopener">paper</a></p>
<p><a href>code</a></p>
<p>该篇文章开篇作者提到要解决的问题就是赋予seq2seq复制的能力，如下所示：</p>
<img src="/2020/06/03/seq2seq-Pointer-Network-Copy-等技术梳理/3.png" title="3.png">

<p>从这个例子中我们可以看到，针对绿色的这部分词汇其实是不需要去理解语意的，直接从输入端copy到输出端就可以了，那我们该如何去实现这个功能呢？，下面我们直接从代码上进行解释：</p>
<h5 id="encode"><a href="#encode" class="headerlink" title="encode"></a>encode</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CopyEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, hidden_size)</span>:</span></span><br><span class="line">        super(CopyEncoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size)</span><br><span class="line"></span><br><span class="line">        self.gru = nn.GRU(input_size=embed_size,</span><br><span class="line">            hidden_size=hidden_size, batch_first=<span class="literal">True</span>,</span><br><span class="line">            bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># input: [b x seq]</span></span><br><span class="line">        embedded = self.embed(x)</span><br><span class="line">        out, h = self.gru(embedded) <span class="comment"># out: [b x seq x hid*2] (biRNN)</span></span><br><span class="line">        <span class="keyword">return</span> out, h</span><br></pre></td></tr></table></figure>

<p>encod部分代码其实不需要进行什么解释了，很容易就理解了</p>
<h5 id="decode"><a href="#decode" class="headerlink" title="decode"></a>decode</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. input_idx 就是decode端第一次的输入，weighted第一次也是初始化的</span></span><br><span class="line">gru_input = torch.cat([self.embed(input_idx).unsqueeze(<span class="number">1</span>), weighted],<span class="number">2</span>) <span class="comment"># [b x 1 x (h*2+emb)]</span></span><br><span class="line">_, state = self.gru(gru_input, prev_state)</span><br><span class="line">state = state.squeeze() <span class="comment"># [b x h]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.拿到state后直接做一个dense得到生成词汇表的概率分布</span></span><br><span class="line">score_g = self.Wo(state) <span class="comment"># [b x vocab_size]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 接着我们需要得到一个copy的概率分布，这个分布当然得根据encode的输出和state一起来计算啦，如下</span></span><br><span class="line">score_c = F.tanh(self.Wc(encoded.contiguous().view(<span class="number">-1</span>,hidden_size*<span class="number">2</span>))) <span class="comment"># [b*seq x hidden_size]</span></span><br><span class="line">score_c = score_c.view(b,<span class="number">-1</span>,hidden_size) <span class="comment"># [b x seq x hidden_size]</span></span><br><span class="line">score_c = torch.bmm(score_c, state.unsqueeze(<span class="number">2</span>)).squeeze() <span class="comment"># [b x seq]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 为了避免padding部分在计算softmax时带来的影响，做了一个mask如下：</span></span><br><span class="line">encoded_mask = torch.Tensor(np.array(encoded_idx==<span class="number">0</span>, dtype=float)*(<span class="number">-1000</span>)) <span class="comment"># [b x seq]</span></span><br><span class="line">score_c = score_c + encoded_mask <span class="comment"># padded parts will get close to 0 when applying softmax</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 将score_g 和 score_c 一起计算softmax：</span></span><br><span class="line"> score = torch.cat([score_g,score_c],<span class="number">1</span>) <span class="comment"># [b x (vocab+seq)]</span></span><br><span class="line"> probs = F.softmax(score)</span><br><span class="line"> prob_g = probs[:,:vocab_size] <span class="comment"># [b x vocab]</span></span><br><span class="line"> prob_c = probs[:,vocab_size:] <span class="comment"># [b x seq]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 此时我们知道 prob_g 和 prob_c 肯定是不能直接相加的，而且 prob_g 也还是一个词汇表的概率分布，但oov时还是不能搞定，</span></span><br><span class="line"><span class="comment"># 所以这里和上一篇文章做法稍微有点不同，这里是固定oov的大小，和上一篇文章动态的变化大小有所区别，其实要改成动态变化也是一样的。</span></span><br><span class="line">oovs = Variable(torch.Tensor(b,self.max_oovs).zero_())+<span class="number">1e-4</span></span><br><span class="line">oovs = self.to_cuda(oovs)</span><br><span class="line">prob_g = torch.cat([prob_g,oovs],<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 而为了要实现可以将prob_g 和 prob_c 相加，做了下面一个操作，encoded_idx 是输入端的一些词汇表表示的数字， </span></span><br><span class="line"><span class="comment"># 初始化一个prob_g大小的one_hot矩阵，根据 encoded_idx 的数字进行补1操作，最后将prob_c 的概率分布放到one_hot </span></span><br><span class="line"><span class="comment"># 上对应的1的位置上，此时，prob_g 和 prob_c 大小是一样的，当然可以直接相加啦</span></span><br><span class="line">en = torch.LongTensor(encoded_idx) <span class="comment"># [b x in_seq]</span></span><br><span class="line">en.unsqueeze_(<span class="number">2</span>) <span class="comment"># [b x in_seq x 1]</span></span><br><span class="line">one_hot = torch.FloatTensor(en.size(<span class="number">0</span>),en.size(<span class="number">1</span>),prob_g.size(<span class="number">1</span>)).zero_() <span class="comment"># [b x in_seq x vocab+oov_nums]</span></span><br><span class="line">one_hot.scatter_(<span class="number">2</span>,en,<span class="number">1</span>) <span class="comment"># one hot tensor: [b x seq x vocab]</span></span><br><span class="line">one_hot = self.to_cuda(one_hot)</span><br><span class="line">prob_c_to_g = torch.bmm(prob_c.unsqueeze(<span class="number">1</span>),Variable(one_hot, requires_grad=<span class="literal">False</span>)) <span class="comment"># [b x 1 x vocab]</span></span><br><span class="line">prob_c_to_g = prob_c_to_g.squeeze() <span class="comment"># [b x vocab]</span></span><br><span class="line"></span><br><span class="line">out = prob_g + prob_c_to_g</span><br><span class="line">out = out.unsqueeze(<span class="number">1</span>) <span class="comment"># [b x 1 x vocab]</span></span><br></pre></td></tr></table></figure>

<p>其实代码后面针对weighted 和 state 还做了一部分操作，这都是次要的，只要理解了out的全部计算过程，可以说CopyNet 的核心思想你也就掌握了，其实这里和上一篇文章没有什么大的差别，这里是直接相加，上一篇文章弄了个软概率来合并，还有一个覆盖操作，大差不差吧。重要的事情说三遍。</p>
<p><strong>阅读code</strong></p>
<p><strong>阅读code</strong></p>
<p><strong>阅读code</strong></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><blockquote>
<p>Seq2seq 的copy机制可以暂时告一段落啦。。。</p>
</blockquote>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Pointer-Network/" rel="tag"># Pointer-Network</a>
          
            <a href="/tags/Summarization/" rel="tag"># Summarization</a>
          
            <a href="/tags/seq2seq/" rel="tag"># seq2seq</a>
          
            <a href="/tags/copy/" rel="tag"># copy</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/05/28/git简单使用记录/" rel="next" title="git简单使用记录">
                <i class="fa fa-chevron-left"></i> git简单使用记录
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/07/31/NLP-训练一个模型-太简单了吧/" rel="prev" title="NLP 训练一个模型,太简单了吧">
                NLP 训练一个模型,太简单了吧 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">王磊</p>
              <p class="site-description motion-element" itemprop="description">浙江理工大学19级，就职于杭州艾耕科技，从事NLP相关工作。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">29</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Pointer-Networks"><span class="nav-number">1.</span> <span class="nav-text">Pointer Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"><span class="nav-number">2.</span> <span class="nav-text">Get To The Point: Summarization with Pointer-Generator Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CopyNet"><span class="nav-number">3.</span> <span class="nav-text">CopyNet</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#encode"><span class="nav-number">3.0.0.1.</span> <span class="nav-text">encode</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#decode"><span class="nav-number">3.0.0.2.</span> <span class="nav-text">decode</span></a></li></ol></li></ol><li class="nav-item nav-level-3"><a class="nav-link" href="#结论"><span class="nav-number">3.1.</span> <span class="nav-text">结论</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">王磊</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
